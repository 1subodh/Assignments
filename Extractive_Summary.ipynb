{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization in Python\n",
    "\n",
    "## Motivation: \n",
    "The length of textual data is increasing and people have less time. Often the newspaper articles run into a long text of, say 1000 -1200 words. As wearable devices leap to prominence (Google Glass, Apple Watch, to name a few), content must adapt to the limited screen space available on these devices.\n",
    "The task of generating intelligent and accurate summaries for long pieces of text has become a popular research as well as industry problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach: \n",
    "Extractive text summarization is all about finding the more important sentences from a document as a summary of that document.\n",
    "Our approach is using the TextRank algorithm to find these 'important' sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'PyPDF2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c31041788a4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# sys library has been used for printing the size of data structures used in the program\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mPyPDF2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdocx2txt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'PyPDF2'"
     ]
    }
   ],
   "source": [
    "# numpy library helps in working with arrays: array creation and manipulation\n",
    "# this implementation uses array for storing the matrices generated as 2-D arrays\n",
    "# PyPDF2 is a library used for reading the PDF files\n",
    "# docx2txt is the library used for reading Word documents \n",
    "# sys library has been used for printing the size of data structures used in the program\n",
    "\n",
    "import numpy as np\n",
    "import PyPDF2\n",
    "import docx2txt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# matplotlib is a library that is used to visualize the data by drawing graphs of matrix inputs\n",
    "# we will use it for drawing the matrices generated later in the program \n",
    "# %matplotlib inline is a command used to show the graphs in the jupyter notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# networkx library helps in working with graphs ...\n",
    "# and later performing the PageRank algorithm ...\n",
    "# which is the crux of this implementation to find ...\n",
    "# the importance of each sentence using their 'rank' as a metric ...\n",
    "# rank, the output of the method textrank, is a measure of importance of sentences\n",
    "# this library has been used in the cell no. ()\n",
    "\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the PunktSentenceTokenizer library is being imported from the file punkt.py contained in package nltk.tokenize \n",
    "# this is used to tokenize the document into sentences\n",
    "# Tokenization: Tokenization is the process of demarcating and possibly classifying.. \n",
    "# sections of a string of input characters. \n",
    "# The resulting tokens are then passed on to some other form of processing. \n",
    "\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TfidfTransformer and CountVectorizer libraries are being imported\n",
    "# CountVectorizer: In this implementation, a CountVectorizer object is being created that ..\n",
    "# will be used for creating the document-term matrix\n",
    "# tFidTransformer: In this implementation,TfidfTransformer is used for executing the method fit_transform()... \n",
    "# which provides the output as a document-term matrix normalized (value 0-1) according to the TF-IDF\n",
    "# TF(Term Frequency): the no. of times a term(a word here) appears in the current document(single sentence here)\n",
    "# IDF(Inverse Document Frequency): the no. of times a term(a word here) appears in the entire corpus\n",
    "# Corpus: set of all sentences\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  Function to read the document from user\n",
    "Supported formats: .txt, .pdf \n",
    "\n",
    "Input: Takes the name of the file as input. \n",
    "\n",
    "Output: Returns a string output containing the contents of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we are going to show an example of how the method is working\n",
    "# first let's take the document as an input\n",
    "def readDoc():\n",
    "    name = input('Please input a file name: ') \n",
    "    print('You have asked for the document {}'.format(name))\n",
    "\n",
    "    # now read the type of document\n",
    "    if name.lower().endswith('.txt'):\n",
    "        choice = 1\n",
    "    elif name.lower().endswith('.pdf'):\n",
    "        choice = 2\n",
    "    else:\n",
    "        choice = 3\n",
    "        # print(name)\n",
    "    print(choice)\n",
    "    # Case 1: if it is a .txt file\n",
    "        \n",
    "    if choice == 1:\n",
    "        f = open(name, 'r')\n",
    "        document = f.read()\n",
    "        f.close()\n",
    "            \n",
    "    # Case 2: if it is a .pdf file\n",
    "    elif choice == 2:\n",
    "        pdfFileObj = open(name, 'rb')\n",
    "        pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "        pageObj = pdfReader.getPage(0)\n",
    "        document = pageObj.extractText()\n",
    "        pdfFileObj.close()\n",
    "    \n",
    "    # Case 3: none of the format\n",
    "    else:\n",
    "        print('Failed to load a valid file')\n",
    "        print('Returning an empty string')\n",
    "        document = ''\n",
    "    \n",
    "    print(type(document))\n",
    "    return document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Function to tokenize the document\n",
    "Input: String of text document\n",
    "\n",
    "Output: A list containing sentences as its elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the function used for tokenizing the sentences\n",
    "# tokenization of a sentence: '''provided in cell() above'''\n",
    "\n",
    "def tokenize(document):\n",
    "    # We are tokenizing using the PunktSentenceTokenizer\n",
    "    # we call an instance of this class as sentence_tokenizer\n",
    "    doc_tokenizer = PunktSentenceTokenizer()\n",
    "    \n",
    "    # tokenize() method: takes our document as input and returns a list of all the sentences in the document\n",
    "    \n",
    "    # sentences is a list containing each sentence of the document as an element\n",
    "    sentences_list = doc_tokenizer.tokenize(document)\n",
    "    return sentences_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Read the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading a file and \n",
    "# printing the size of the file\n",
    "document = readDoc()\n",
    "print('The length of the file is:', end=' ')\n",
    "print(len(document))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Generate a list of sentences in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to tokenize the document for further processing\n",
    "# tokenizing the sentence means that we are creating a list of all the sentences of the document.\n",
    "# Need of tokenizing the document: Initially the document is in just a string format.\n",
    "# if we want to process the document, we need to store it in a data structure.\n",
    "# Tokenization of document into words is also possible, but we will go with the tokenizing with the sentences\n",
    "# Since we want to choose the most relevant sentences, we need to generate tokens of sentences only\n",
    "sentences_list = tokenize(document)\n",
    "\n",
    "# let us print the size of memory used by the list sentences\n",
    "print('The size of the list in Bytes is: {}'.format(sys.getsizeof(sentences_list)))\n",
    "\n",
    "# the size of one of the element of the list\n",
    "print('The size of the item 0 in Bytes is: {}'.format(sys.getsizeof(sentences_list[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us see the data type of sentences_list\n",
    "# It will be list\n",
    "print(type(sentences_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us analyse the elements of the sentences\n",
    "# len() method applies on the list and provides the number of elements in the list\n",
    "print('The size of the list \"sentences\" is: {}'.format(len(sentences_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the elements of the list\n",
    "# If the input document is long, which on realistically will be wrong, we would not like to print the entire document\n",
    "for i in sentences_list:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Generate term-document matrix (TD matrix) of the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert a collection of text documents to a matrix of token counts\n",
    "# fit_transform method of CountVectorizer() class \n",
    "# Learn the vocabulary dictionary and return term-document matrix. \n",
    "# I/p: An iterable which yields either str, unicode or file objects.\n",
    "# O/p: The term-document matrix named cv_matrix\n",
    "cv = CountVectorizer()\n",
    "cv_matrix = cv.fit_transform(sentences_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**So what does CountVectorizer.fit_transform() do?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a demo of what CountVectorizer().fit_transform(text) does\n",
    "cv_demo = CountVectorizer() # a demo object of class CountVectorizer\n",
    "\n",
    "# I have repeated the words to make a non-ambiguous array of the document text matrix \n",
    "\n",
    "text_demo = [\"Ashish is good, you are bad\", \"I am not bad\"] \n",
    "res_demo = cv_demo.fit_transform(text_demo)\n",
    "print('Result demo array is {}'.format(res_demo.toarray()))\n",
    "\n",
    "# Result is 2-d matrix containing document text matrix\n",
    "# Notice that in the second row, there is 2.\n",
    "# also, bad is repeated twice in that sentence.\n",
    "# so we can infer that 2 is corresponding to the word 'bad'\n",
    "print('Feature list: {}'.format(cv_demo.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the cv_matrix type\n",
    "# and how it is being stored in memory?\n",
    "# it is stored in the compressed row format\n",
    "# compressed row format: \n",
    "print('The data type of bow matrix {}'.format(type(cv_matrix)))\n",
    "print('Shape of the matrix {}'.format(cv_matrix.get_shape))\n",
    "print('Size of the matrix is: {}'.format(sys.getsizeof(cv_matrix)))\n",
    "print(cv.get_feature_names())\n",
    "print(cv_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tnormalized: document-term matrix normalized (value 0-1) according to the TF-IDF\n",
    "# TF(Term Frequency): the no. of times a term(a word here) appears in the current document(single sentence here)\n",
    "# IDF(Inverse Document Frequency): the no. of times a term(a word here) appears in the entire corpus\n",
    "# Corpus: set of all sentences\n",
    "\n",
    "normal_matrix = TfidfTransformer().fit_transform(cv_matrix)\n",
    "print(normal_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(normal_matrix.T.toarray)\n",
    "res_graph = normal_matrix * normal_matrix.T\n",
    "# plt.spy(res_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drawing a graph to proceed for the textrank algorithm\n",
    "# nx_graph is a graph developed using the networkx library\n",
    "# each node represents a sentence\n",
    "# an edge represents that they have words in common\n",
    "# the edge weight is the number of words that are common in both of the sentences(nodes)\n",
    "# nx.draw() method is used to draw the graph created\n",
    "\n",
    "nx_graph = nx.from_scipy_sparse_matrix(res_graph)\n",
    "nx.draw_circular(nx_graph)\n",
    "print('Number of edges {}'.format(nx_graph.number_of_edges()))\n",
    "print('Number of vertices {}'.format(nx_graph.number_of_nodes()))\n",
    "plt.show()\n",
    "print('The memory used by the graph in Bytes is: {}'.format(sys.getsizeof(nx_graph)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  note that the graph above is dense and therefor it resembles a circle\n",
    "# if a shorter document is taken, a beautiful circular graph can be seen "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Getting the rank of every sentence using textrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ranks is a dictionary with key=node(sentences) and value=textrank (the rank of each of the sentences)\n",
    "ranks = nx.pagerank(nx_graph)\n",
    "\n",
    "# analyse the data type of ranks\n",
    "print(type(ranks))\n",
    "print('The size used by the dictionary in Bytes is: {}'.format(sys.getsizeof(ranks)))\n",
    "\n",
    "# print the dictionary\n",
    "for i in ranks:\n",
    "    print(i, ranks[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Finding important sentences and generating summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# enumerate method: returns an enumerate object\n",
    "# Use of list Comprehensions\n",
    "# O/p: sentence_array is the sorted(descending order w.r.t. score value) 2-d array of ranks[sentence] and sentence \n",
    "# For example, if there are two sentences: S1 (with a score of S1 = s1) and S2 with score s2, with s2>s1\n",
    "# then sentence_array is [[s2, S2], [s1, S1]]\n",
    "sentence_array = sorted(((ranks[i], s) for i, s in enumerate(sentences_list)), reverse=True)\n",
    "sentence_array = np.asarray(sentence_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# as sentence_array is in descending order wrt score value\n",
    "# fmax is the largest score value(the score of first element)\n",
    "# fmin is the smallest score value(the score of last element)\n",
    "\n",
    "rank_max = float(sentence_array[0][0])\n",
    "rank_min = float(sentence_array[len(sentence_array) - 1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the largest and smallest value of scores of the sentence\n",
    "print(rank_max)\n",
    "print(rank_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization of the scores\n",
    "# so that it comes out in the range 0-1\n",
    "# fmax becomes 1\n",
    "# fmin becomes 0\n",
    "# store the normalized values in the list temp_array\n",
    "\n",
    "temp_array = []\n",
    "\n",
    "# if all sentences have equal ranks, means they are all the same\n",
    "# taking any sentence will give the summary, say the first sentence\n",
    "flag = 0\n",
    "if rank_max - rank_min == 0:\n",
    "    temp_array.append(0)\n",
    "    flag = 1\n",
    "\n",
    "# If the sentence has different ranks\n",
    "if flag != 1:\n",
    "    for i in range(0, len(sentence_array)):\n",
    "        temp_array.append((float(sentence_array[i][0]) - rank_min) / (rank_max - rank_min))\n",
    "\n",
    "print(len(temp_array))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculation of threshold:\n",
    "# We take the mean value of normalized scores\n",
    "# any sentence with the normalized score 0.2 more than the mean value is considered to be \n",
    "threshold = (sum(temp_array) / len(temp_array)) + 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Separate out the sentences that satiasfy the criteria of having a score above the threshold\n",
    "sentence_list = []\n",
    "if len(temp_array) > 1:\n",
    "    for i in range(0, len(temp_array)):\n",
    "        if temp_array[i] > threshold:\n",
    "                sentence_list.append(sentence_array[i][1])\n",
    "else:\n",
    "    sentence_list.append(sentence_array[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = sentence_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Writing the summary to a new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(sentence_list)\n",
    "summary = \" \".join(str(x) for x in sentence_list)\n",
    "print(summary)\n",
    "# save the data in another file, names sum.txt\n",
    "f = open('final3.txt', 'a+')\n",
    "#print(type(f))\n",
    "f.write('\\n')\n",
    "f.write(summary)\n",
    "f.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for lines in sentence_list:\n",
    "    print(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please feel free to contribue for any improvements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
